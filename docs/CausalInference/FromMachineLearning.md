

## Problem Definition

é‡‡ç”¨Neyman-Rubin çš„æ½œåœ¨ç»“æœçš„æ¨¡å‹æ¡†æ¶(POF)ï¼Œå¹¶å‡è®¾ä¸€ä¸ªè¶…æ€»ä½“ä¸Šçš„åˆ†å¸ƒ Pä¸Šäº§å‡ºN ä¸ªç‹¬ç«‹çš„æ ·æœ¬ã€‚ X è¡¨ç¤ºåå˜é‡ï¼ŒYè¡¨ç¤ºä¸¤ç§å¹²é¢„ä¸‹çš„æ½œåœ¨ç»“æœï¼ŒTè¡¨ç¤ºå¹²é¢„çš„indicatorã€‚
$$
\displaylines{
(Y_i(0),Y_i(1),X_i,T_i) \sim P \\
ATE := E[Y(1) - Y(0)] \\
\mu_0(X) := E[Y(0)|X=x]\text{    and   } \mu_1(X) := E[Y(0)|X=x]
}
$$


ITE or CATE


$$
\displaylines{
\tau(x) := E[Y(1) - Y(0)| X=x]}
$$



 ### DGP å‡è®¾

The model makes the following structural equation assumptions on the data generating process


$$
\begin{split}Y =~& \theta(X) \cdot T + g(X, T) + \epsilon ~~~&~~~ E[\epsilon | X, T] = 0 \\
T =~& f(X, T) + \eta & E[\eta \mid X, T] = 0 \\
~& E[\eta \cdot \epsilon | X, T] = 0\end{split}
$$




## Challenages Or Bias in Causal Inference

é‚£å…¶å®æœ€æ ¸å¿ƒçš„é—®é¢˜å°±æ˜¯ï¼Œä¸ºä»€ä¹ˆæŸäº›æ–¹æ³•å¤±æ•ˆï¼Œ æˆ‘ä»¬è§£å†³çš„è¿™ä¸ªé—®é¢˜æœ¬èº«æœ‰å“ªäº›å›°éš¾ç‚¹ï¼Œå“ªäº›è¯¯å·®ç‚¹ï¼Œ æˆ‘ä»¬æ€ä¹ˆå»è§£å†³ã€‚



å»ºæ¨¡æ€è·¯çš„é—®é¢˜

1.  ä¸ºä»€ä¹ˆæ™®é€šçš„å»ºæ¨¡ä¸èƒ½åšï¼Œç±»ä¼¼indirect estimationï¼Œdirection estimation ä¸­çš„transformç­‰
2.  ä¸ºä»€ä¹ˆè¦å°†CI ç»“åˆML è¿›è¡Œï¼Ÿ
3.  ä¸ºä»€ä¹ˆè¦å¼•å…¥PLR modelï¼Œè€Œä¸”è¿˜ä¿ç•™ç€Dï¼Œ PLR çš„å‡è®¾æœ‰ä»€ä¹ˆä¼˜è¶Šæ€§
4.  ä¸ºä»€ä¹ˆå°†PLR model æ”¹æˆæ®‹å·®çš„å½¢æ€ï¼Œç„¶åè¿›è¡Œä¸¤æ­¥å»ºæ¨¡
5.  åˆ°åº•è¿™é‡Œçš„DMLæ–¹æ³•æœ‰ä»€ä¹ˆä¼˜ç‚¹åœ¨å¯¹PLR ä¸­çš„
6.  å„ç§å› æœæ¨æ–­çš„æ ‘æ–¹æ³•æœ‰å•¥å·®å¼‚
7.  æ­£åˆ™åå·®ï¼ˆ ä¸¤ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹çš„bias çš„å’Œï¼Œå’Œä¸¤ä¸ªæœºå™¨å­¦ä¹ bias çš„ç§¯ï¼‰



there is no other source of difference between treatment and untreated other than the treatment itself

> The key observation for understanding this phenomena is that gâ‚€(Z)â‰ ğ”¼[Y|Z]. Thus, it is not possible in general to get a good estimate of gâ‚€(Z) by â€œregressingâ€ Y on Z, and this in turn leads to the impossibility of getting a good estimate of Î¸â‚€. It is perfectly possible, nevertheless, to get very good predictions of Y given Z and D. This is why we usually say that Machine Learning is good for prediction, but bad for causal inference. 



#### 4.3.2 Native Approach  From Machine Learning

è¿™é‡Œæœ‰ä¸¤ä¸ªå‚æ•°ï¼Œ target parameter theta, nuisence paramter g

å¯ä»¥ä½¿ç”¨iterative method çš„æ–¹æ³•ä½¿ç”¨éšæœºæ£®æ—å»ä¼°è®¡g_0 ,ç„¶åä½¿ç”¨ols å»å›å½’ $\theta_0$

ä»OLS çš„æ–¹å¼çœ‹ é‚£ä¹ˆå¦‚æœæ˜¯å¯¹$\theta_0$ è¿›è¡Œæ±‚è§£ï¼Œç›´æ¥å¯ä»¥å¾—åˆ°
$$
argmin_{\theta_{0}} [Y - D\hat \theta_0 - g(x)]^2
$$

$$
2*D * [Y - D\hat\theta_0 -g(x)] = 0 = DY - D^2\hat\theta_0 - Dg(x) \\
then \\
\hat \theta_0 = [D^2]^{-1}D(Y - \hat g(x)) \\
\hat g(x) = Y - D\hat \theta_0 \\
$$

ä½†æ˜¯è¿™é‡Œè¿˜æœ‰ä¸€ä¸ª$$g_0(x) $$ ä¹Ÿæ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå¹¶ä¸”å’Œx ç›¸å…³ï¼Œ æ‰€ä»¥éœ€è¦å°†g_0(x) å…ˆæ±‚è§£å‡ºæ¥, æ‰€ä»¥è¿™ä¸ªg(x) å’Œ$\theta$  éƒ½æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œ å¦‚æœä»MSE ä¼˜åŒ–çš„è§’åº¦æ˜¯å¯ä»¥ç›´æ¥è¿›è¡Œæ¨å¯¼çš„ï¼Œ é—®é¢˜å°±æ˜¯æ— åæ€§çš„ä¿è¯ï¼Œå’Œæ¸è¿›ä¸€è‡´æ€§çš„ä¿è¯äº†ã€‚

è¦è¯æ˜Native Approach çš„ä¼°è®¡çš„æœ‰å
$$
\begin{aligned}
\hat \theta = [D^2]^{-1}D(Y - \hat g(x))  &= [D^2]^{-1}D(D\theta_0(X) + g(x) +U - \hat g(x)) \\
&= \theta_0   +  [D^2]^{-1}D(g(x) +U - \hat g(x)) \\
&=\left( \frac{1}{N}\sum_{i=1}^N D_i^2\right)^{-1}\frac{1}{n}\sum D_i(Y_i - \hat g_0(X_i))\\
\end{aligned}
$$

so

$$
\begin{aligned}
\hat\theta - \theta \\ &= [D^2]^{-1}[(g(x) +U - \hat g(x))] \\&= [D^2]^{-1}DU + [D^2]^{-1}D(g(x)- \hat g_0(x))\\ &=
\left( \frac{1}{N}\sum_{i=1}^N D_i^2\right)^{-1} \frac{1}{n}DU_i + \left( \frac{1}{N}\sum_{i=1}^N D_i^2\right)^{-1}\sum_{i=1}\frac{1}{n}D_i(g_0(x_i) - \hat g_0(x_i)) \\
&= a + b \\
&=
 (ED^2)^{-1}\frac{1}{n}DU_i + (ED^2)^{-1}\sum_{i=1}\frac{1}{n}m_i(g_0(x_i) - \hat g_0(x_i)) +(ED^2)^{-1}\sum_{i=1}\frac{1}{n}v_i(g_0(x_i) - \hat g_0(x_i)) \\
 & =a^* + b^* + c^*
\end{aligned}
$$

so
$$
E[\hat\theta - \theta]  = [D^2]^{-1}E[(g(x) +U - \hat g(x))]  = [D^2]^{-1}E[(g(x) - \hat g(x))] \\
Var[\hat\theta - \theta]  = ([D^2]^{-1})^2Var[(g(x) +U - \hat g(x))]
$$

æ‰€ä»¥è¿™é‡Œæ˜¯å¦æ— åï¼Œå–å†³äºg(x) çš„é¢„ä¼°æ˜¯å¦æ— åï¼Œ å¦‚æœg(x) æ˜¯é¢„ä¼°æ— åçš„ï¼Œé‚£ä¹ˆ$\theta$ ä¹Ÿæ˜¯é¢„ä¼°æ— åçš„ï¼Œ é‚£ä¹ˆè¿™é‡Œçš„g(x) ç¬¬ä¸€æ­¥æ±‚å‡ºæ¥æ˜¯è‚¯å®šæ˜¯æœ‰åå·®çš„ï¼Œ æ˜¯å¦è¦åå¤æ”¶æ•›ï¼Œ é‚£ä¹ˆè¿™é‡Œä¼šå‡ºç°é—®é¢˜ï¼Œè¿™é‡Œinterest å˜é‡ä¼°è®¡çš„æ— åæ€§å’Œæ¸è¿›ä¸€è‡´æ€§éƒ½æ˜¯ä¾èµ–äºnuseriance çš„å‚æ•°çš„ä¼°è®¡çš„æ€§è´¨ã€‚ é‚£å¦‚ä½•èƒ½å¤Ÿç¡®å®šè¿™ä¸ªå˜é‡ä¼°è®¡æ˜¯å¯¹çš„å‘¢ã€‚

**å¦‚æœæ˜¯ä½¿ç”¨OLS æ±‚è§£ï¼Œæ•´ä½“çš„è¯¯å·®å…¨åœ¨g(x) ä¸Šï¼Œå¦‚æœg(x) çš„æ±‚è§£æ˜¯æœ‰åçš„ï¼Œé‚£ä¹ˆç™½çäº†ï¼Œè¯¯å·®åªä¼šæ‰©å¤§**ã€‚



æˆ‘ä»¬çœ‹C Term ä¹Ÿå¯ä»¥å‘ç°

<img src="../../../Paper/çŸ¥è¯†æ€»ç»“/å› æœæ¨æ–­/Causal_ml.assets/image-20210618211942080.png" alt="image-20210618211942080" style="zoom:30%;" />



 

**Challenges  In Double Machine Learning**

ä»è¿™ä¸ªnative approach çš„è§’åº¦è¯æ˜ä¸‹ï¼Œç›´æ¥è¿›è¡Œä¼°è®¡æ˜¯å­˜åœ¨åå·®çš„ã€‚åœ¨è¿™ä¸ªDGP å’Œé—®é¢˜å‡è®¾ä¸‹ï¼Œ è¦å¾—åˆ°ä¸€ä¸ªæ— åä¼°è®¡é‡ï¼Œéœ€è¦å¤„ç†biasd  çš„g(x) ã€‚ å®é™…ä¸Šè¿™äº›åå·®æ€»ç»“èµ·æ¥å°±æ˜¯ä¸¤ç§æ¥æºã€‚é«˜ç»´å¤æ‚æ¨¡å‹ä¸‹çš„æ­£åˆ™æŸå¤±ï¼Œ ä»¥åŠè¿‡æ‹Ÿåˆã€‚  ä¾‹å¦‚ä¸Šè¿°çš„B è¯¯å·®å¯¹åº”å¤æ‚æ¨¡å‹g(x) ä¸‹çš„è¿‡æ‹Ÿåˆï¼Œä»è€Œå¯¼è‡´è¯¯å·®è¢«æ”¾å¤§ã€‚

The bias has two sources, regularization and overfitting.

- Bias from regularization

  to avoid overfitting the data with complex functional forms, ML use regularization. This decrease the variance of the estimator and reduces overfitting but introduces bias and prevents sqrt n consistency

  å®é™…å¯¹åº”å“ªä¸ªï¼Ÿ

- Bias from overfitting
  sometimes our efforts to regularize fail to prevent overfitting( mistaking noise for signal
   	bias and slow convergence )



è§£å†³çš„æ–¹æ¡ˆ

regularization bias by means of orthogonalization 

overfitting bias by means of cross-fitting. 